import { InputValues, MemoryVariables, OutputValues } from "./base.js";
import { BaseChatMemory, BaseChatMemoryInput } from "./chat_memory.js";
import { BaseLanguageModel } from "../base_language/index.js";
/**
 * Interface for the input parameters of the `BufferTokenMemory` class.
 */
export interface ConversationTokenBufferMemoryInput extends BaseChatMemoryInput {
    humanPrefix?: string;
    aiPrefix?: string;
    llm: BaseLanguageModel;
    memoryKey?: string;
    maxTokenLimit?: number;
}
/**
 * Class that represents a conversation chat memory with a token buffer.
 * It extends the `BaseChatMemory` class and implements the
 * `ConversationTokenBufferMemoryInput` interface.
 */
export declare class ConversationTokenBufferMemory extends BaseChatMemory implements ConversationTokenBufferMemoryInput {
    humanPrefix: string;
    aiPrefix: string;
    memoryKey: string;
    maxTokenLimit: number;
    llm: BaseLanguageModel;
    constructor(fields: ConversationTokenBufferMemoryInput);
    get memoryKeys(): string[];
    /**
     * Loads the memory variables. It takes an `InputValues` object as a
     * parameter and returns a `Promise` that resolves with a
     * `MemoryVariables` object.
     * @param _values `InputValues` object.
     * @returns A `Promise` that resolves with a `MemoryVariables` object.
     */
    loadMemoryVariables(_values: InputValues): Promise<MemoryVariables>;
    /**
     * Saves the context from this conversation to buffer. If the amount
     * of tokens required to save the buffer exceeds MAX_TOKEN_LIMIT,
     * prune it.
     */
    saveContext(inputValues: InputValues, outputValues: OutputValues): Promise<void>;
}
